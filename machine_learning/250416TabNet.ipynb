{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d83d0eb4-462d-4fa8-b158-ef8ed9811f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "84c98b70-f01e-4a84-b62e-c8a4b0ae2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§© Attention + ì˜ˆì¸¡ ëª¨ë‘ manual_weights ë°˜ì˜\n",
    "# ì…ë ¥ íŠ¹ì„±ë§ˆë‹¤ ìˆ˜ë™ ê°€ì¤‘ì¹˜ë¥¼ ë”í•´ì¤Œìœ¼ë¡œì¨ íŠ¹ì • íŠ¹ì„±ì˜ ì¤‘ìš”ë„ë¥¼ ê°•ì¡°í•  ìˆ˜ ìˆìŒ\n",
    "class CustomAttentiveTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, manual_weights=None):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, input_dim)  # ì…ë ¥ íŠ¹ì„± ìˆ˜ë§Œí¼ attention ì ìˆ˜ ê³„ì‚°í•˜ëŠ” fully connected layer\n",
    "        self.manual_weights = manual_weights  # ì‚¬ìš©ìê°€ ì§€ì •í•œ ìˆ˜ë™ ê°€ì¤‘ì¹˜\n",
    "\n",
    "    def forward(self, x, prior):\n",
    "        raw_scores = self.fc(x)  # ì…ë ¥ìœ¼ë¡œë¶€í„° ê¸°ë³¸ attention ì ìˆ˜ë¥¼ ìƒì„±\n",
    "        if self.manual_weights is not None:\n",
    "            # manual_weightsë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ì—¬ attention scoreì— ë”í•¨\n",
    "            manual_tensor = torch.tensor(self.manual_weights, dtype=torch.float32).to(x.device)\n",
    "            raw_scores = raw_scores + manual_tensor  # âœ… ì„ íƒ ê°•ì¡°: ìˆ˜ë™ ê°€ì¤‘ì¹˜ë¡œ íŠ¹ì • íŠ¹ì„±ì˜ ì„ íƒ í™•ë¥  ì¦ê°€\n",
    "        mask = F.softmax(raw_scores * prior, dim=-1)  # priorì™€ ê³±í•˜ê³  softmaxë¡œ ì •ê·œí™”í•˜ì—¬ ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        return mask  # ìµœì¢… attention ë§ˆìŠ¤í¬ ë°˜í™˜\n",
    "\n",
    "\n",
    "# âœ… TabNet êµ¬ì¡° ì •ì˜ (ê°„ì†Œí™” ë²„ì „)\n",
    "# attention ë§ˆìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì§•ì„ ì„ íƒí•˜ê³ , ê° ìŠ¤í…ì˜ ë³€í™˜ ê²°ê³¼ë¥¼ ëˆ„ì í•´ ì˜ˆì¸¡í•¨\n",
    "class CustomTabNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_steps=3, decision_dim=64, manual_weights=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_steps = n_steps  # ë°˜ë³µ íšŸìˆ˜ (feature selection ë° transformation)\n",
    "        self.decision_dim = decision_dim  # ê° ìŠ¤í…ì—ì„œ ìƒì„±ë˜ëŠ” ì¤‘ê°„ í‘œí˜„ í¬ê¸°\n",
    "        self.manual_weights = manual_weights  # ì‚¬ìš©ìê°€ ì§ì ‘ ì •ì˜í•œ íŠ¹ì„± ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜\n",
    "\n",
    "        # attention ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•  ì—¬ëŸ¬ ìŠ¤í…ì˜ attentive transformer ë ˆì´ì–´ ì •ì˜\n",
    "        self.attentive_layers = nn.ModuleList([\n",
    "            CustomAttentiveTransformer(input_dim, manual_weights=manual_weights)\n",
    "            for _ in range(n_steps)\n",
    "        ])\n",
    "\n",
    "        # ì„ íƒëœ íŠ¹ì§•ì— ëŒ€í•´ ë¹„ì„ í˜• ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” feature transformer ë ˆì´ì–´\n",
    "        self.feature_transformers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, decision_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(decision_dim, decision_dim),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(n_steps)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(decision_dim, 1)  # ì¶œë ¥ì¸µ (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prior = torch.ones_like(x)  # priorëŠ” ê° íŠ¹ì„±ì˜ ì‚¬ìš© ì •ë„ë¥¼ ê´€ë¦¬í•˜ëŠ” ë²¡í„° (ì´ˆê¸°ì—ëŠ” ëª¨ë“  íŠ¹ì„±ì´ ë™ì¼ ë¹„ì¤‘)\n",
    "        outputs = []  # ìŠ¤í…ë³„ ì¶œë ¥ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            mask = self.attentive_layers[step](x, prior)  # í˜„ì¬ ìŠ¤í…ì—ì„œì˜ attention ë§ˆìŠ¤í¬ ìƒì„±\n",
    "            x_att = x * mask  # ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•´ ì„ íƒëœ íŠ¹ì„±ë§Œ ë°˜ì˜\n",
    "\n",
    "            if self.manual_weights is not None:\n",
    "                # âœ… ì˜ˆì¸¡ ë°˜ì˜ ê°•ì¡°: ì„ íƒëœ ì…ë ¥ì— manual_weightsë¥¼ í•œ ë²ˆ ë” ê³±í•¨ (ì˜ˆì¸¡ì— ì˜í–¥ì„ ì£¼ë„ë¡)\n",
    "                manual_tensor = torch.tensor(self.manual_weights, dtype=torch.float32).to(x.device)\n",
    "                x_att = x_att * manual_tensor\n",
    "\n",
    "            transformed = self.feature_transformers[step](x_att)  # ì„ íƒëœ ì…ë ¥ì„ ë³€í™˜\n",
    "            outputs.append(transformed)  # ë³€í™˜ ê²°ê³¼ ì €ì¥\n",
    "            prior = prior * (1 - mask)  # ì„ íƒëœ íŠ¹ì„±ì€ ë‹¤ìŒ ìŠ¤í…ì—ì„œ ëœ ì‚¬ìš©ë˜ë„ë¡ prior ì—…ë°ì´íŠ¸\n",
    "\n",
    "        agg = torch.sum(torch.stack(outputs), dim=0)  # ëª¨ë“  ìŠ¤í…ì˜ ì¶œë ¥ í•©ì‚°\n",
    "        return torch.sigmoid(self.output_layer(agg))  # ìµœì¢… ì´ì§„ í™•ë¥ ê°’ ì¶œë ¥\n",
    "\n",
    "\n",
    "# âœ… í•™ìŠµ í•¨ìˆ˜: BCE loss ê¸°ì¤€ìœ¼ë¡œ ìµœì í™”\n",
    "# ê²€ì¦ ë°ì´í„°ì˜ PR AUC ê¸°ì¤€ìœ¼ë¡œ early stopping ìˆ˜í–‰\n",
    "def train_model(model, X_train, y_train, X_val, y_val, lr=1e-3, epochs=100, patience=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if (isinstance(X_train, torch.Tensor) and isinstance(X_val, torch.Tensor)):\n",
    "        X_train = X_train.to(device)\n",
    "        X_val = X_val.to(device)\n",
    "    else:\n",
    "        X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32).to(device)\n",
    "        X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32).to(device)\n",
    "\n",
    "    if (isinstance(y_train, torch.Tensor) and isinstance(y_val, torch.Tensor)):\n",
    "        y_train = y_train.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "    else:\n",
    "        y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32).to(device)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()  # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ binary cross entropy ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš©\n",
    "\n",
    "    best_auc = 0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_train).squeeze()  # í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡\n",
    "        loss = criterion(preds, y_train)  # ì†ì‹¤ ê³„ì‚°\n",
    "        loss.backward()  # ì—­ì „íŒŒ\n",
    "        optimizer.step()  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "\n",
    "        # ê²€ì¦ ë°ì´í„°ë¡œ PR AUC ì¸¡ì •\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val).squeeze().detach().cpu().numpy()\n",
    "            val_labels = y_val.detach().cpu().numpy()\n",
    "            pr_auc = average_precision_score(val_labels, val_preds)\n",
    "\n",
    "        # best ëª¨ë¸ ì €ì¥ ë° patience ì¡°ê±´ ì²´í¬\n",
    "        if pr_auc > best_auc:\n",
    "            best_auc = pr_auc\n",
    "            best_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break  # patience ì´ˆê³¼ ì‹œ í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)  # ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ë¡œ ë³µì›\n",
    "\n",
    "\n",
    "# âœ… í‰ê°€ í•¨ìˆ˜: ROC AUC, PR AUC ê³„ì‚° (ê²€ì¦ or í…ŒìŠ¤íŠ¸ìš©)\n",
    "def evaluate_model(model, X, y):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (isinstance(X, torch.Tensor) and isinstance(y, torch.Tensor)):\n",
    "        None\n",
    "    else:\n",
    "        X = torch.tensor(X.to_numpy(), dtype=torch.float32).to(device)\n",
    "        y = torch.tensor(y.to_numpy(), dtype=torch.float32).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = model(X).squeeze().cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "    roc = roc_auc_score(labels, probs)\n",
    "    pr = average_precision_score(labels, probs)\n",
    "    return roc, pr\n",
    "\n",
    "\n",
    "# âœ… êµì°¨ê²€ì¦ í•¨ìˆ˜: ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ K-fold PR AUC / ROC AUC í‰ê·  ì„±ëŠ¥ ì¸¡ì •\n",
    "def cross_validate_model(X, y, manual_weights=None, k=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    roc_list, pr_list = [], []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train = torch.tensor(X[train_idx], dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y[train_idx], dtype=torch.float32).to(device)\n",
    "        X_val = torch.tensor(X[val_idx], dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y[val_idx], dtype=torch.float32).to(device)\n",
    "\n",
    "        model = CustomTabNet(input_dim=X.shape[1], manual_weights=manual_weights)\n",
    "        train_model(model, X_train, y_train, X_val, y_val)\n",
    "        roc, pr = evaluate_model(model, X_val, y_val)\n",
    "        roc_list.append(roc)\n",
    "        pr_list.append(pr)\n",
    "\n",
    "    # í‰ê·  ë° í‘œì¤€í¸ì°¨ ì¶œë ¥\n",
    "    print(f\"ROC AUC (mean Â± std): {np.mean(roc_list):.4f} Â± {np.std(roc_list):.4f}\")\n",
    "    print(f\"PR  AUC (mean Â± std): {np.mean(pr_list):.4f} Â± {np.std(pr_list):.4f}\")\n",
    "    return roc_list, pr_list\n",
    "\n",
    "# âœ… ëœë¤ ì„œì¹˜ í•¨ìˆ˜: ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ì¤‘ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§„ êµ¬ì„± íƒìƒ‰\n",
    "\n",
    "def random_search(X, y, n_trials=10, param_grid=None):\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_steps': [2, 3, 4],  # ìŠ¤í… ìˆ˜\n",
    "            'decision_dim': [32, 64, 128],  # ì¤‘ê°„ í‘œí˜„ ì°¨ì›\n",
    "            'lr': [1e-3, 5e-4],  # í•™ìŠµë¥ \n",
    "            'patience': [5, 10],  # early stopping ê¸°ë‹¤ë¦¼\n",
    "        }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # ëœë¤í•˜ê²Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ì„ íƒ\n",
    "        config = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        print(f\"Trial {trial+1}/{n_trials} | Config: {config}\")\n",
    "        roc_scores, pr_scores = [], []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train = torch.tensor(X[train_idx], dtype=torch.float32).to(device)\n",
    "            y_train = torch.tensor(y[train_idx], dtype=torch.float32).to(device)\n",
    "            X_val = torch.tensor(X[val_idx], dtype=torch.float32).to(device)\n",
    "            y_val = torch.tensor(y[val_idx], dtype=torch.float32).to(device)\n",
    "\n",
    "            model = CustomTabNet(\n",
    "                input_dim=X.shape[1],\n",
    "                n_steps=config['n_steps'],\n",
    "                decision_dim=config['decision_dim']\n",
    "            )\n",
    "            train_model(model, X_train, y_train, X_val, y_val,\n",
    "                        lr=config['lr'], patience=config['patience'])\n",
    "\n",
    "            roc, pr = evaluate_model(model, X_val, y_val)\n",
    "            roc_scores.append(roc)\n",
    "            pr_scores.append(pr)\n",
    "\n",
    "        avg_roc = np.mean(roc_scores)\n",
    "        avg_pr = np.mean(pr_scores)\n",
    "        results.append((config, avg_roc, avg_pr))\n",
    "\n",
    "    # PR AUC ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ë°˜í™˜\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    for i, (cfg, roc, pr) in enumerate(results):\n",
    "        print(f\"Rank {i+1}: PR AUC = {pr:.4f}, ROC AUC = {roc:.4f} | Config: {cfg}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0312ac8c-8f41-41f7-b853-901a58372138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ì‹œë“œ ê³ ì • ì½”ë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë©´ ë¨)\n",
    "def set_seed(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # í•´ì‹œ ë¬´ì‘ìœ„ì„± ì œê±°\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # ì—¬ëŸ¬ GPU ì“¸ ê²½ìš°\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c49e384c-13d4-4cb4-996b-81b0dd245b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (mean Â± std): 0.4956 Â± 0.0416\n",
      "PR  AUC (mean Â± std): 0.1586 Â± 0.0183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5, 0.5679140951954117, 0.5, 0.4481161963249988, 0.46197969976117365],\n",
       " [0.14659685863874344,\n",
       "  0.19220159783593532,\n",
       "  0.1467248908296943,\n",
       "  0.16377347087382643,\n",
       "  0.1435115605020268])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… 3. êµì°¨ê²€ì¦ ì„±ëŠ¥ í‰ê°€\n",
    "cross_validate_model(X_train.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cec286a5-44ad-4f1b-8b5d-a1e838ebb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('data/(ìµœì¢…)_ì„œìš¸ì—´ì„ _ê´‘ì§„ë„ë¡œ.csv')\n",
    "X = pd.get_dummies(data[['ë„ë¡œ ì¢…ë¥˜', 'ë„ë¡œí­', 'ê²½ì‚¬ê°', 'ìµœê·¼ì ‘_ì‹œì„¤ì˜_í‰ê· ê±°ë¦¬', 'ì¢…í•©_í‰ê· _ê¸°ì˜¨', 'ìƒí™œì¸êµ¬', 'ìµœê·¼ì ‘_ì‹œì„¤ë“¤_ìµœì†Œê±°ë¦¬', 'ìµœê·¼ì ‘_ì‹œì„¤ë“¤_ìµœëŒ€ê±°ë¦¬']])\n",
    "y = data['ì—´ì„ ']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.astype('float')\n",
    "X_test = X_test.astype('float')\n",
    "X = X.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1223b521-8c44-4cff-9087-fd70844d4cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5878029771646793, 0.22561407283699864)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì‚¬ìš©ì˜ˆì‹œ\n",
    "\n",
    "# TabNetClassifier ì´ˆê¸°í™”\n",
    "model = CustomTabNet(input_dim=15)\n",
    "\n",
    "# í•™ìŠµ ì „ ìˆ˜ë™ ê°€ì¤‘ì¹˜ ì„¤ì •\n",
    "# manual_weights = np.array([0.0, 1.5, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# ëª¨ë¸ ë‚´ë¶€ Attention Layerì— ê°€ì¤‘ì¹˜ ì „ë‹¬\n",
    "# CustomAttentiveTransformer(15, manual_weights)\n",
    "\n",
    "# fit í˜¸ì¶œ\n",
    "train_model(model, X_train=X_train, y_train=y_train, X_val = X_test, y_val = y_test)\n",
    "evaluate_model(model, X= X_test, y= y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2644a6fa-9ff8-4bed-b5ab-1f31f3d741a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.71511968e-01,  1.36137368e+02, -1.74322200e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.15915578e+00,  3.66472346e+02, -1.87292600e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 9.96087770e+00,  1.36263448e+02, -2.04503300e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 1.61736308e-01,  3.88010033e+02, -1.61081200e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 5.48780740e+00,  2.74204454e+02, -1.47322000e-01, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 3.24288688e-02,  2.01955198e+02, -1.14795500e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bb114-85bc-473c-b03f-a595ca4b3023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62c93bf9-4c4c-4585-b41d-9890176e3201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TabNetClassifier.weight_updater of TabNetClassifier(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=0, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=None, output_dim=None, device_name='auto', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])>\n"
     ]
    }
   ],
   "source": [
    "print(model.weight_updater)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
