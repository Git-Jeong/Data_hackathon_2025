{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d83d0eb4-462d-4fa8-b158-ef8ed9811f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "84c98b70-f01e-4a84-b62e-c8a4b0ae2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧩 Attention + 예측 모두 manual_weights 반영\n",
    "# 입력 특성마다 수동 가중치를 더해줌으로써 특정 특성의 중요도를 강조할 수 있음\n",
    "class CustomAttentiveTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, manual_weights=None):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, input_dim)  # 입력 특성 수만큼 attention 점수 계산하는 fully connected layer\n",
    "        self.manual_weights = manual_weights  # 사용자가 지정한 수동 가중치\n",
    "\n",
    "    def forward(self, x, prior):\n",
    "        raw_scores = self.fc(x)  # 입력으로부터 기본 attention 점수를 생성\n",
    "        if self.manual_weights is not None:\n",
    "            # manual_weights를 텐서로 변환하여 attention score에 더함\n",
    "            manual_tensor = torch.tensor(self.manual_weights, dtype=torch.float32).to(x.device)\n",
    "            raw_scores = raw_scores + manual_tensor  # ✅ 선택 강조: 수동 가중치로 특정 특성의 선택 확률 증가\n",
    "        mask = F.softmax(raw_scores * prior, dim=-1)  # prior와 곱하고 softmax로 정규화하여 마스크 생성\n",
    "        return mask  # 최종 attention 마스크 반환\n",
    "\n",
    "\n",
    "# ✅ TabNet 구조 정의 (간소화 버전)\n",
    "# attention 마스크를 기반으로 특징을 선택하고, 각 스텝의 변환 결과를 누적해 예측함\n",
    "class CustomTabNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_steps=3, decision_dim=64, manual_weights=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_steps = n_steps  # 반복 횟수 (feature selection 및 transformation)\n",
    "        self.decision_dim = decision_dim  # 각 스텝에서 생성되는 중간 표현 크기\n",
    "        self.manual_weights = manual_weights  # 사용자가 직접 정의한 특성 중요도 가중치\n",
    "\n",
    "        # attention 마스크를 생성할 여러 스텝의 attentive transformer 레이어 정의\n",
    "        self.attentive_layers = nn.ModuleList([\n",
    "            CustomAttentiveTransformer(input_dim, manual_weights=manual_weights)\n",
    "            for _ in range(n_steps)\n",
    "        ])\n",
    "\n",
    "        # 선택된 특징에 대해 비선형 변환을 수행하는 feature transformer 레이어\n",
    "        self.feature_transformers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, decision_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(decision_dim, decision_dim),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(n_steps)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(decision_dim, 1)  # 출력층 (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prior = torch.ones_like(x)  # prior는 각 특성의 사용 정도를 관리하는 벡터 (초기에는 모든 특성이 동일 비중)\n",
    "        outputs = []  # 스텝별 출력 저장 리스트\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            mask = self.attentive_layers[step](x, prior)  # 현재 스텝에서의 attention 마스크 생성\n",
    "            x_att = x * mask  # 마스크를 적용해 선택된 특성만 반영\n",
    "\n",
    "            if self.manual_weights is not None:\n",
    "                # ✅ 예측 반영 강조: 선택된 입력에 manual_weights를 한 번 더 곱함 (예측에 영향을 주도록)\n",
    "                manual_tensor = torch.tensor(self.manual_weights, dtype=torch.float32).to(x.device)\n",
    "                x_att = x_att * manual_tensor\n",
    "\n",
    "            transformed = self.feature_transformers[step](x_att)  # 선택된 입력을 변환\n",
    "            outputs.append(transformed)  # 변환 결과 저장\n",
    "            prior = prior * (1 - mask)  # 선택된 특성은 다음 스텝에서 덜 사용되도록 prior 업데이트\n",
    "\n",
    "        agg = torch.sum(torch.stack(outputs), dim=0)  # 모든 스텝의 출력 합산\n",
    "        return torch.sigmoid(self.output_layer(agg))  # 최종 이진 확률값 출력\n",
    "\n",
    "\n",
    "# ✅ 학습 함수: BCE loss 기준으로 최적화\n",
    "# 검증 데이터의 PR AUC 기준으로 early stopping 수행\n",
    "def train_model(model, X_train, y_train, X_val, y_val, lr=1e-3, epochs=100, patience=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if (isinstance(X_train, torch.Tensor) and isinstance(X_val, torch.Tensor)):\n",
    "        X_train = X_train.to(device)\n",
    "        X_val = X_val.to(device)\n",
    "    else:\n",
    "        X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32).to(device)\n",
    "        X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32).to(device)\n",
    "\n",
    "    if (isinstance(y_train, torch.Tensor) and isinstance(y_val, torch.Tensor)):\n",
    "        y_train = y_train.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "    else:\n",
    "        y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32).to(device)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()  # 이진 분류를 위한 binary cross entropy 손실함수 사용\n",
    "\n",
    "    best_auc = 0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_train).squeeze()  # 학습 데이터에 대한 예측\n",
    "        loss = criterion(preds, y_train)  # 손실 계산\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 파라미터 업데이트\n",
    "\n",
    "        # 검증 데이터로 PR AUC 측정\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val).squeeze().detach().cpu().numpy()\n",
    "            val_labels = y_val.detach().cpu().numpy()\n",
    "            pr_auc = average_precision_score(val_labels, val_preds)\n",
    "\n",
    "        # best 모델 저장 및 patience 조건 체크\n",
    "        if pr_auc > best_auc:\n",
    "            best_auc = pr_auc\n",
    "            best_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break  # patience 초과 시 학습 조기 종료\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)  # 가장 성능이 좋았던 모델로 복원\n",
    "\n",
    "\n",
    "# ✅ 평가 함수: ROC AUC, PR AUC 계산 (검증 or 테스트용)\n",
    "def evaluate_model(model, X, y):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (isinstance(X, torch.Tensor) and isinstance(y, torch.Tensor)):\n",
    "        None\n",
    "    else:\n",
    "        X = torch.tensor(X.to_numpy(), dtype=torch.float32).to(device)\n",
    "        y = torch.tensor(y.to_numpy(), dtype=torch.float32).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = model(X).squeeze().cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "    roc = roc_auc_score(labels, probs)\n",
    "    pr = average_precision_score(labels, probs)\n",
    "    return roc, pr\n",
    "\n",
    "\n",
    "# ✅ 교차검증 함수: 주어진 데이터에 대해 K-fold PR AUC / ROC AUC 평균 성능 측정\n",
    "def cross_validate_model(X, y, manual_weights=None, k=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    roc_list, pr_list = [], []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train = torch.tensor(X[train_idx], dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y[train_idx], dtype=torch.float32).to(device)\n",
    "        X_val = torch.tensor(X[val_idx], dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y[val_idx], dtype=torch.float32).to(device)\n",
    "\n",
    "        model = CustomTabNet(input_dim=X.shape[1], manual_weights=manual_weights)\n",
    "        train_model(model, X_train, y_train, X_val, y_val)\n",
    "        roc, pr = evaluate_model(model, X_val, y_val)\n",
    "        roc_list.append(roc)\n",
    "        pr_list.append(pr)\n",
    "\n",
    "    # 평균 및 표준편차 출력\n",
    "    print(f\"ROC AUC (mean ± std): {np.mean(roc_list):.4f} ± {np.std(roc_list):.4f}\")\n",
    "    print(f\"PR  AUC (mean ± std): {np.mean(pr_list):.4f} ± {np.std(pr_list):.4f}\")\n",
    "    return roc_list, pr_list\n",
    "\n",
    "# ✅ 랜덤 서치 함수: 다양한 하이퍼파라미터 조합 중 가장 좋은 성능을 가진 구성 탐색\n",
    "\n",
    "def random_search(X, y, n_trials=10, param_grid=None):\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_steps': [2, 3, 4],  # 스텝 수\n",
    "            'decision_dim': [32, 64, 128],  # 중간 표현 차원\n",
    "            'lr': [1e-3, 5e-4],  # 학습률\n",
    "            'patience': [5, 10],  # early stopping 기다림\n",
    "        }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # 랜덤하게 하이퍼파라미터 조합 선택\n",
    "        config = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        print(f\"Trial {trial+1}/{n_trials} | Config: {config}\")\n",
    "        roc_scores, pr_scores = [], []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train = torch.tensor(X[train_idx], dtype=torch.float32).to(device)\n",
    "            y_train = torch.tensor(y[train_idx], dtype=torch.float32).to(device)\n",
    "            X_val = torch.tensor(X[val_idx], dtype=torch.float32).to(device)\n",
    "            y_val = torch.tensor(y[val_idx], dtype=torch.float32).to(device)\n",
    "\n",
    "            model = CustomTabNet(\n",
    "                input_dim=X.shape[1],\n",
    "                n_steps=config['n_steps'],\n",
    "                decision_dim=config['decision_dim']\n",
    "            )\n",
    "            train_model(model, X_train, y_train, X_val, y_val,\n",
    "                        lr=config['lr'], patience=config['patience'])\n",
    "\n",
    "            roc, pr = evaluate_model(model, X_val, y_val)\n",
    "            roc_scores.append(roc)\n",
    "            pr_scores.append(pr)\n",
    "\n",
    "        avg_roc = np.mean(roc_scores)\n",
    "        avg_pr = np.mean(pr_scores)\n",
    "        results.append((config, avg_roc, avg_pr))\n",
    "\n",
    "    # PR AUC 기준으로 내림차순 정렬하여 상위 결과 반환\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    for i, (cfg, roc, pr) in enumerate(results):\n",
    "        print(f\"Rank {i+1}: PR AUC = {pr:.4f}, ROC AUC = {roc:.4f} | Config: {cfg}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0312ac8c-8f41-41f7-b853-901a58372138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 시드 고정 코드 (한 번만 실행하면 됨)\n",
    "def set_seed(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 해시 무작위성 제거\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # 여러 GPU 쓸 경우\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c49e384c-13d4-4cb4-996b-81b0dd245b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (mean ± std): 0.4956 ± 0.0416\n",
      "PR  AUC (mean ± std): 0.1586 ± 0.0183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5, 0.5679140951954117, 0.5, 0.4481161963249988, 0.46197969976117365],\n",
       " [0.14659685863874344,\n",
       "  0.19220159783593532,\n",
       "  0.1467248908296943,\n",
       "  0.16377347087382643,\n",
       "  0.1435115605020268])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 3. 교차검증 성능 평가\n",
    "cross_validate_model(X_train.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cec286a5-44ad-4f1b-8b5d-a1e838ebb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('data/(최종)_서울열선_광진도로.csv')\n",
    "X = pd.get_dummies(data[['도로 종류', '도로폭', '경사각', '최근접_시설의_평균거리', '종합_평균_기온', '생활인구', '최근접_시설들_최소거리', '최근접_시설들_최대거리']])\n",
    "y = data['열선']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.astype('float')\n",
    "X_test = X_test.astype('float')\n",
    "X = X.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1223b521-8c44-4cff-9087-fd70844d4cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5878029771646793, 0.22561407283699864)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용예시\n",
    "\n",
    "# TabNetClassifier 초기화\n",
    "model = CustomTabNet(input_dim=15)\n",
    "\n",
    "# 학습 전 수동 가중치 설정\n",
    "# manual_weights = np.array([0.0, 1.5, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# 모델 내부 Attention Layer에 가중치 전달\n",
    "# CustomAttentiveTransformer(15, manual_weights)\n",
    "\n",
    "# fit 호출\n",
    "train_model(model, X_train=X_train, y_train=y_train, X_val = X_test, y_val = y_test)\n",
    "evaluate_model(model, X= X_test, y= y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2644a6fa-9ff8-4bed-b5ab-1f31f3d741a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.71511968e-01,  1.36137368e+02, -1.74322200e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.15915578e+00,  3.66472346e+02, -1.87292600e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 9.96087770e+00,  1.36263448e+02, -2.04503300e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 1.61736308e-01,  3.88010033e+02, -1.61081200e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 5.48780740e+00,  2.74204454e+02, -1.47322000e-01, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 3.24288688e-02,  2.01955198e+02, -1.14795500e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bb114-85bc-473c-b03f-a595ca4b3023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62c93bf9-4c4c-4585-b41d-9890176e3201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TabNetClassifier.weight_updater of TabNetClassifier(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=0, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=None, output_dim=None, device_name='auto', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])>\n"
     ]
    }
   ],
   "source": [
    "print(model.weight_updater)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
